# ì¸í„°í˜ì´ìŠ¤.py
# ------------------------------------------------------------
# ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ë¶„ì„ ì‹œìŠ¤í…œ (RSS ì „ìš©, í”„ë¡¬í”„íŠ¸ ë³´ê°•íŒ)
# - ë³´ì•ˆë‰´ìŠ¤ RSS ìˆ˜ì§‘
# - KoELECTRA NER ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ + ì—…ì¢…ë³„ ê°€ì¤‘ì¹˜ ë¶„ì„
# - CISA KEV feed ë°˜ì˜
# - Gemini ê¸°ë°˜ ìš”ì•½/í”Œë ˆì´ë¶ ìƒì„± (message.txt ì˜ë„ ë°˜ì˜ í”„ë¡¬í”„íŠ¸)
# - PDF ë³´ê³ ì„œ, ëŒ€ì‹œë³´ë“œ/ë‰´ìŠ¤/í”Œë ˆì´ë¶ íƒ­
# - LLM ì „ë‹¬ í‚¤ì›Œë“œ ë° Prompt/Response ë¡œê·¸ ë…¸ì¶œ (ì œê±°)
# ------------------------------------------------------------

import os
import re
import time
import json
import requests
import pandas as pd
import streamlit as st
import feedparser
import torch
import textwrap

from bs4 import BeautifulSoup
from datetime import datetime
from fpdf import FPDF
from dotenv import load_dotenv
from transformers import ElectraTokenizerFast, ElectraForTokenClassification
import google.generativeai as genai

# ============================================================
# 0) ê³µí†µ ì„¤ì •
# ============================================================
st.set_page_config(
    page_title="ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ê³µìš© ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        color: white;
        text-align: center;
    }
    .recommendation-box {
        background: linear-gradient(135deg, #f8f9ff 0%, #e8f2ff 100%);
        padding: 2rem;
        border-radius: 10px;
        margin: 1rem 0;
        border: 1px solid #d4e6ff;
    }
    .sidebar-logo {
        text-align: center;
        padding: 1rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
        margin-bottom: 1rem;
        color: white;
    }
    .stSelectbox > div > div { background-color: #f8f9fa; }
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        width: 100%;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 4px 4px 12px rgba(102, 126, 234, 0.4);
    }
    .news-item {
        background: white;
        padding: 1.5rem;
        border-radius: 8px;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        border-left: 3px solid #3498db;
    }
    .risk-high { border-left-color: #e74c3c !important; }
    .risk-medium { border-left-color: #f39c12 !important; }
    .risk-low { border-left-color: #27ae60 !important; }
</style>
""", unsafe_allow_html=True)

# ============================================================
# 1) í™˜ê²½ ë³€ìˆ˜/LLM ì„¤ì •
# ============================================================
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    st.error("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()
genai.configure(api_key=GEMINI_API_KEY)

GENERATION_CONFIG = {
    "temperature": 0.5,
    "max_output_tokens": 1400,
}
GEMINI = genai.GenerativeModel('gemini-1.5-flash', generation_config=GENERATION_CONFIG)

# ============================================================
# 2) ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ (RSS ì „ìš©)
# ============================================================
def scrape_article(url: str):
    """ë³´ì•ˆë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ìŠ¤í¬ë˜í•‘ (íƒ€ì´í‹€/ë³¸ë¬¸/ì¼ì)"""
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=7)
        if res.status_code != 200:
            return None
        soup = BeautifulSoup(res.text, "html.parser")

        # ë‹¤ì–‘í•œ í…œí”Œë¦¿ ëŒ€ì‘
        title = soup.select_one("#news_title02") or soup.select_one("h4.tit")
        body = soup.select_one("#news_content") or soup.select_one("div.view_txt")
        date = soup.select_one("#news_util01") or soup.select_one("span.date")

        return {
            "url": url,
            "title": title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ",
            "date": date.get_text(strip=True) if date else datetime.now().strftime("%Y-%m-%d"),
            "content": body.get_text("\n", strip=True) if body else "ë‚´ìš© ì—†ìŒ",
            "source": "ë³´ì•ˆë‰´ìŠ¤"
        }
    except Exception:
        return None

def fetch_latest_news_by_rss():
    """ë³´ì•ˆë‰´ìŠ¤ RSS ì—¬ëŸ¬ í”¼ë“œì—ì„œ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘"""
    rss_list = [
        ("SECURITY", "http://www.boannews.com/media/news_rss.xml?mkind=1"),
        ("IT", "http://www.boannews.com/media/news_rss.xml?mkind=2"),
        ("SAFETY", "http://www.boannews.com/media/news_rss.xml?mkind=4"),
        ("ì‚¬ê±´ã†ì‚¬ê³ ", "http://www.boannews.com/media/news_rss.xml?kind=1"),
        ("ê³µê³µã†ì •ì±…", "http://www.boannews.com/media/news_rss.xml?kind=2"),
        ("ë¹„ì¦ˆë‹ˆìŠ¤", "http://www.boannews.com/media/news_rss.xml?kind=3"),
        ("êµ­ì œ", "http://www.boannews.com/media/news_rss.xml?kind=4"),
        ("í…Œí¬", "http://www.boannews.com/media/news_rss.xml?kind=5"),
    ]
    seen_urls, seen_titles, all_articles = set(), set(), []
    for feed_name, rss_url in rss_list:
        try:
            feed = feedparser.parse(rss_url)
        except Exception:
            continue
        for entry in getattr(feed, "entries", []):
            url = getattr(entry, "link", None)
            title = getattr(entry, "title", "").strip()
            if not url or url in seen_urls or title in seen_titles:
                continue
            seen_urls.add(url)
            seen_titles.add(title)
            data = scrape_article(url)
            if data and data['title'] != 'ì œëª© ì—†ìŒ':
                data["source"] = feed_name
                all_articles.append(data)
            time.sleep(0.1)
    return all_articles

# ============================================================
# 3) NER ëª¨ë¸ ë¡œë”© (ê°€ëŠ¥ì‹œ) ë° ë¶„ì„ í´ë°±
# ============================================================
@st.cache_resource(show_spinner=False)
def load_ner_model():
    """
    KoELECTRA NER ëª¨ë¸ ë¡œë”©.
    ë¡œì»¬ ê²½ë¡œì— í•™ìŠµëœ ëª¨ë¸ì´ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ (tokenizer/model) None ë°˜í™˜.
    """
    try:
        NER_MODEL_PATH = os.getenv("KOELECTRA_NER_PATH", "").strip()
        if not NER_MODEL_PATH:
            return None, None, None
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = ElectraTokenizerFast.from_pretrained(NER_MODEL_PATH)
        model = ElectraForTokenClassification.from_pretrained(NER_MODEL_PATH).to(device).eval()
        id2label = model.config.id2label
        return tokenizer, model, (device, id2label)
    except Exception:
        return None, None, None

ner_tokenizer, ner_model, ner_ctx = load_ner_model()

def ner_inference(sentence: str):
    """NER ê¸°ë°˜ í† í°â†’ì›Œë“œ ì¬êµ¬ì„± í›„ ë¼ë²¨ O ì œì™¸ í† í° ë°˜í™˜"""
    if not (ner_tokenizer and ner_model and ner_ctx):
        return []
    device, id2label = ner_ctx
    enc = ner_tokenizer(sentence, return_offsets_mapping=True,
                         return_tensors="pt", truncation=True)
    word_ids = enc.word_ids()
    with torch.no_grad():
        outputs = ner_model(input_ids=enc["input_ids"].to(device),
                            attention_mask=enc["attention_mask"].to(device))
        pred_ids = torch.argmax(outputs.logits, dim=-1)[0].cpu().tolist()

    results, cur_word, cur_label, prev_wid = [], "", None, None
    for idx, wid in enumerate(word_ids):
        if wid is None:
            continue
        tok_id = int(enc["input_ids"][0, idx].item())
        piece = ner_tokenizer.convert_ids_to_tokens(tok_id)
        label = ner_ctx[1][int(pred_ids[idx])]
        if wid != prev_wid:
            if cur_word:
                results.append((cur_word, cur_label))
            cur_word, cur_label = piece.replace("##", ""), label
        else:
            cur_word += piece.replace("##", "")
        prev_wid = wid
    if cur_word:
        results.append((cur_word, cur_label))
    return [w for w, l in results if l != "O"]

industry_risk_map = {
    "IT/ì†Œí”„íŠ¸ì›¨ì–´": {
        "ëœì„¬ì›¨ì–´":1.0,"ì œë¡œë°ì´":1.0,"ì·¨ì•½ì ":1.0,"API":0.9,"í´ë¼ìš°ë“œ":1.0,"SaaS":0.9,"DevOps":0.9,"GitHub":0.9,
        "ì˜¤í”ˆì†ŒìŠ¤":0.9,"CVE":1.0,"íŒ¨ì¹˜":1.0,"ìµìŠ¤í”Œë¡œì‡":1.0,"ê³µê¸‰ë§ ê³µê²©":1.0,"ì†Œí”„íŠ¸ì›¨ì–´ ì—…ë°ì´íŠ¸":0.9,"ì•…ì„±ì½”ë“œ":1.0,
        "ë°±ë„ì–´":1.0,"ì›¹ì‰˜":0.9,"SQL ì¸ì ì…˜":1.0,"XSS":1.0,"CSRF":0.9,"SSRF":0.9,"XXE":0.9,"IDOR":0.9,
        "ë²„í¼ ì˜¤ë²„í”Œë¡œìš°":1.0,"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜":0.8,"ê¶Œí•œ ìƒìŠ¹":0.9,"ì„¸ì…˜ í•˜ì´ì¬í‚¹":0.9,"ì„¸ì…˜ ê³ ì •":0.8,"ì·¨ì•½í•œ ì•”í˜¸í™”":1.0,
        "í•˜ë“œì½”ë”©ëœ í‚¤":0.9,"í‰ë¬¸ ì „ì†¡":0.9,"API í‚¤ ë…¸ì¶œ":1.0,"í¬ë¦¬ë´ì…œ ìŠ¤í„°í•‘":1.0,"ë¸Œë£¨íŠ¸í¬ìŠ¤":0.9,"ë”•ì…”ë„ˆë¦¬ ê³µê²©":0.9,
        "í”¼ì‹±":0.8,"ìŠ¤í”¼ì–´í”¼ì‹±":0.9,"ì›Œí„°ë§í™€":0.8,"APT":1.0,"ì‚¬íšŒê³µí•™":0.9,"ì•…ì„± ìŠ¤í¬ë¦½íŠ¸":0.8,"ì›¹ ì·¨ì•½ì ":0.9,
        "ì½”ë“œ ì„œëª… ìœ„ì¡°":0.9,"ì·¨ì•½í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬":0.9,"npm íŒ¨í‚¤ì§€ ê³µê²©":0.9,"PyPI ê³µê²©":0.9,"ë§¬ì›¨ì–´":1.0,
        "RAT":0.9,"ë£¨íŠ¸í‚·":0.9,"ë¡œì§ë°¤":0.8,"ë´‡ë„·":0.9,"ì›œ":0.9,"ë°”ì´ëŸ¬ìŠ¤":0.9,
        "CI/CD ê³µê²©":0.9,"ì»¨í…Œì´ë„ˆ íƒˆì¶œ":0.9,"ì¿ ë²„ë„¤í‹°ìŠ¤ ê³µê²©":0.9,"ë„ì»¤ í—ˆë¸Œ ì•…ì„± ì´ë¯¸ì§€":0.8,
        "IaC ë³´ì•ˆ":0.8,"ì·¨ì•½í•œ ì„¤ì •":0.9,"ì˜ëª»ëœ ê¶Œí•œ":0.9,"IAM ì˜¤ìš©":0.9,"S3 ë²„í‚· ë…¸ì¶œ":1.0,
        "RaaS":0.8,"ì„œë¹„ìŠ¤ ê±°ë¶€":0.8,"DoS":0.8,"DDoS":1.0,"í´ë¼ìš°ë“œ ê¶Œí•œ ìƒìŠ¹":0.9,"MITM":0.9,"DNS ìŠ¤í‘¸í•‘":0.9,
        "íŒ¨í‚· ìŠ¤ë‹ˆí•‘":0.9,"VPN ê³µê²©":0.9,"í† í° íƒˆì·¨":0.9,"ì„¸ì…˜ íƒˆì·¨":0.9,"ì´ë©”ì¼ ê³„ì • íƒˆì·¨":0.9,
        "APT ê³µê²©":1.0,"ë¶í•œ í•´í‚¹":1.0,"ì¤‘êµ­ í•´í‚¹":1.0,"ë¼ìë£¨ìŠ¤":1.0,"ê¹€ìˆ˜í‚¤":1.0,"APT37":0.9,"APT28":0.9,
        "ë³´ì•ˆ ì„¤ì • ë¯¸í¡":0.9,"ê¶Œí•œ ì˜¤ë‚¨ìš©":0.9,"ì•”í˜¸í™” ë¯¸ì ìš©":1.0,"ë°ì´í„° ìœ ì¶œ":1.0,"ì†ŒìŠ¤ì½”ë“œ ìœ ì¶œ":1.0,
        "DevSecOps ë¯¸í¡":0.9,"ì·¨ì•½í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ":0.8,"ë³´ì•ˆ ìë™í™” ë¶€ì¬":0.8,"ì·¨ì•½ì  ìŠ¤ìºë‹ ëˆ„ë½":0.9,
        "ë³´ì•ˆ ë¡œê·¸ ë¯¸ìˆ˜ì§‘":0.9,"SIEM ë¶€ì¬":0.9,"EDR ë¯¸ì ìš©":0.9,"MFA ë¯¸ì ìš©":1.0,"ì•½í•œ ë¹„ë°€ë²ˆí˜¸":1.0,
        "OAuth ì·¨ì•½ì ":0.9,"SSO ìš°íšŒ":0.9,"JWT ë³€ì¡°":0.9,"GraphQL ê³µê²©":0.9,"NoSQL ì¸ì ì…˜":0.9,
        "API ê²Œì´íŠ¸ì›¨ì´ ìš°íšŒ":0.9,"í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ê³µê²©":0.9,"IaC ìŠ¤ìº” ëˆ„ë½":0.8,"CSP ì„¤ì • ì˜¤ë¥˜":0.8,
        "ì•ˆì „í•˜ì§€ ì•Šì€ ë¦¬ë‹¤ì´ë ‰íŠ¸":0.8,"ì„¸ì…˜ í† í° ì¬ì‚¬ìš©":0.9,"ì¿ í‚¤ íƒˆì·¨":0.9,"ë¸Œë¼ìš°ì € ìµìŠ¤í”Œë¡œì‡":0.9,
        "ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸ ë¯¸ì ìš©":0.9,"ë§ ë¶„ë¦¬ ìš°íšŒ":0.9,"ë³´ì•ˆ ëª¨ë‹ˆí„°ë§ ë¶€ì¬":0.9,
        "Prompt Injection":1.0,"LLM Jailbreak":1.0,"ë°ì´í„° í¬ì´ì¦ˆë‹":1.0,"AI ëª¨ë¸ ë„ìš©":1.0,
        "AI í™˜ê°":0.9,"ëª¨ë¸ ì—­ì¶”ì ":1.0,"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë…¸ì¶œ":1.0
    },
    "ì œì¡°ì—…": {
        "ì‚°ì—…ì œì–´ì‹œìŠ¤í…œ":1.0,"SCADA":1.0,"ICS":1.0,"PLC":1.0,"ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬":1.0,"ë¡œë´‡":0.9,"CNC":0.9,"HMI":1.0,
        "ì‚°ì—…ìš© IoT":0.9,"OT ë³´ì•ˆ":1.0,"ì œì¡° ë¼ì¸ ê³µê²©":1.0,"ê³µê¸‰ë§ ê³µê²©":1.0,"ì•…ì„± USB":0.8,
        "ëœì„¬ì›¨ì–´":1.0,"íŠ¸ë¡œì´ëª©ë§ˆ":0.9,"ì›Œí„°ë§í™€":0.8,"APT":1.0,"ìŠ¤í”¼ì–´í”¼ì‹±":0.8,"ì‚¬íšŒê³µí•™":0.8,
        "ë°ì´í„° ìœ ì¶œ":1.0,"ìƒì‚° ì°¨ì§ˆ":1.0,"ë¡œë´‡ í•´í‚¹":0.9,"CVE":1.0,"ì œë¡œë°ì´":1.0,"ì•…ì„±ì½”ë“œ":1.0,
        "VPN ê³µê²©":0.9,"MITM":0.8,"DoS":0.8,"DDoS":0.9,"ë²„í¼ ì˜¤ë²„í”Œë¡œìš°":0.9,"ë©”ëª¨ë¦¬ ì·¨ì•½ì ":0.9,
        "ê¶Œí•œ ìƒìŠ¹":0.9,"ì„¸ì…˜ í•˜ì´ì¬í‚¹":0.8,"ë°±ë„ì–´":0.9,"ë´‡ë„·":0.8,"ì›œ":0.8,"ë£¨íŠ¸í‚·":0.8,"RAT":0.8,
        "ìŠ¤ë§ˆíŠ¸ì„¼ì„œ":0.9,"IoT ë³´ì•ˆ":0.9,"íŒì›¨ì–´ í•´í‚¹":0.9,"ì·¨ì•½í•œ ì•”í˜¸í™”":0.9,"í•˜ë“œì½”ë”©ëœ í‚¤":0.8,"í‰ë¬¸ í†µì‹ ":0.8,
        "ì›ê²© ì½”ë“œ ì‹¤í–‰":0.9,"SQL ì¸ì ì…˜":0.7,"XSS":0.7,"SSRF":0.7,"CSRF":0.7,"ì›¹ ì·¨ì•½ì ":0.7,
        "ê³µì¥ ìë™í™” ê³µê²©":1.0,"ì œì¡° ë°ì´í„° ìœ„ì¡°":1.0,"ìŠ¤íŒŒì´ì›¨ì–´":0.8,"ì‚°ì—… ìŠ¤íŒŒì´":1.0,"ì„¤ë¹„ íŒŒê´´":1.0,
        "ìœ„ì¡° ë¶€í’ˆ":1.0,"ë¶ˆëŸ‰í’ˆ ì£¼ì…":1.0,"ìƒì‚° ì¤‘ë‹¨":1.0,"êµ­ê°€ ì§€ì› í•´í‚¹":1.0,"ë¼ìë£¨ìŠ¤":1.0,
        "APT41":1.0,"ê¸°ê³„ ì œì–´ ì·¨ì•½ì ":1.0,"ì‚°ì—… ë„¤íŠ¸ì›Œí¬ ì¹¨íˆ¬":1.0,"ë³´ì•ˆ ì„¤ì • ë¯¸í¡":0.9,
        "ì ‘ê·¼ í†µì œ ì‹¤íŒ¨":0.9,"ë°ì´í„° ë¬´ê²°ì„± ê³µê²©":1.0,"ìœ„ì¡° ì¸ì¦ì„œ":0.9,"ì¸ì¦ ìš°íšŒ":0.9,"ì•…ì„± íŒì›¨ì–´":1.0,
        "Modbus ê³µê²©":1.0,"DNP3 ê³µê²©":1.0,"HMI ìœ„ì¡°":0.9,"ì‚°ì—… ë¡œë´‡ ì œì–´ê¶Œ íƒˆì·¨":1.0,
        "ì—ë„ˆì§€ ê´€ë¦¬ì‹œìŠ¤í…œ ê³µê²©":1.0,"PLC ë¡œì§ ì£¼ì…":1.0,"ì‚°ì—… ë„¤íŠ¸ì›Œí¬ ìŠ¤ë‹ˆí•‘":0.9,"ë§ë¶„ë¦¬ ìš°íšŒ":0.9,
        "ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œ ê³µê²©":1.0,"ì‚°ì—…ìš© ë¬´ì„  ì¹¨íˆ¬":0.8,"ë””ì§€í„¸ íŠ¸ìœˆ í•´í‚¹":0.9,
        "AI ê¸°ë°˜ ì œì¡° ê³µê²©":0.7,"AI ëª¨ë¸ ìœ„ì¡°":0.7,"í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜":0.6
    },
    "ê¸ˆìœµì—…": {
        "í”¼ì‹±":1.0,"ìŠ¤í”¼ì–´í”¼ì‹±":1.0,"ì´ë©”ì¼ ê³„ì • íƒˆì·¨":1.0,"ê³„ì •ì •ë³´ ìœ ì¶œ":1.0,"í¬ë¦¬ë´ì…œ ìŠ¤í„°í•‘":1.0,
        "ë¸Œë£¨íŠ¸í¬ìŠ¤":1.0,"ë”•ì…”ë„ˆë¦¬ ê³µê²©":1.0,"ê³„ì¢Œ íƒˆì·¨":1.0,"ì€í–‰":1.0,"ì¹´ë“œì‚¬":1.0,"ê²°ì œì •ë³´ ìœ ì¶œ":1.0,
        "ì•”í˜¸í™”í":1.0,"ê±°ë˜ì†Œ í•´í‚¹":1.0,"DeFi ê³µê²©":1.0,"í•€í…Œí¬":0.9,"ì˜¤í”ˆë±…í‚¹":1.0,"API í‚¤ ë…¸ì¶œ":1.0,
        "ëœì„¬ì›¨ì–´":0.9,"íŠ¸ë¡œì´ëª©ë§ˆ":0.9,"ì•…ì„±ì½”ë“œ":0.9,"ë´‡ë„·":0.9,"RAT":0.9,"ë£¨íŠ¸í‚·":0.8,
        "APT":1.0,"ì‚¬íšŒê³µí•™":1.0,"BEC":1.0,"ê°€ì§œ ì•±":0.9,"ëª¨ë°”ì¼ í”¼ì‹±":1.0,"ìŠ¤ë¯¸ì‹±":1.0,"QR í”¼ì‹±":1.0,
        "DDoS":1.0,"ì„œë¹„ìŠ¤ ê±°ë¶€":0.9,"MITM":1.0,"DNS ìŠ¤í‘¸í•‘":0.9,"íŒ¨í‚· ìŠ¤ë‹ˆí•‘":0.9,
        "ì•…ì„± ê²°ì œ ëª¨ë“ˆ":1.0,"ë°±ë„ì–´":0.9,"ì •ë³´ íƒˆì·¨":1.0,"ë°ì´í„° ìœ ì¶œ":1.0,
        "ê³ ê°ì •ë³´ ìœ ì¶œ":1.0,"ê¸ˆìœµì‚¬ê¸°":1.0,"ë³´ì´ìŠ¤í”¼ì‹±":1.0,"ê°€ì§œ íˆ¬ì":1.0,"ë¼ìë£¨ìŠ¤":1.0,"ê¹€ìˆ˜í‚¤":1.0,
        "APT38":1.0,"êµ­ê°€ ì§€ì› í•´í‚¹":1.0,"SWIFT ê³µê²©":1.0,"ATM í•´í‚¹":0.9,"POS ê³µê²©":0.9,
        "í•€í…Œí¬ API ì·¨ì•½ì ":0.9,"ì•”í˜¸í™” ë¯¸ì ìš©":1.0,"ì•½í•œ ë¹„ë°€ë²ˆí˜¸":1.0,"2FA ë¯¸ì ìš©":1.0,
        "ì„¸ì…˜ íƒˆì·¨":0.9,"í† í° íƒˆì·¨":0.9,"ë¶ˆë²• ì†¡ê¸ˆ":1.0,"ì•…ì„± ë´‡":0.9,"ë”¥í˜ì´í¬ ì‚¬ê¸°":1.0,
        "ëŒ€ì¶œ ì‚¬ê¸°":1.0,"ê°€ì§œ ë³´í—˜":0.9,"ëª¨ë°”ì¼ ë±…í‚¹ ì•…ì„±ì•±":1.0,"í•€í…Œí¬ SDK ì·¨ì•½ì ":0.9,
        "ì•”í˜¸í™”í íƒˆì·¨":1.0,"í”¼ì‹± ì›¹ì‚¬ì´íŠ¸":1.0,"ê°€ì§œ ì¸ì¦ì„œ":0.9,"MFA í”¼ì‹±":1.0,
        "CBDC ìœ„í˜‘":0.9,"ì•”í˜¸í™”í ê±°ë˜ì†Œ ë‚´ë¶€ì ê³µê²©":1.0,
        "AI ê¸ˆìœµ ì‚¬ê¸°":1.0,"í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜":0.9,"AI ì±—ë´‡ í”¼ì‹±":1.0,"AI ë”¥í˜ì´í¬":1.0
    },
    "ì˜ë£Œì—…": {
        "í™˜ìì •ë³´":1.0,"ì˜ë£Œê¸°ê¸°":1.0,"IoMT":1.0,"ì˜ë£Œ ë°ì´í„° ìœ ì¶œ":1.0,"ë³‘ì› í•´í‚¹":1.0,"EMR":1.0,"EHR":1.0,
        "ì›ê²©ì§„ë£Œ":1.0,"ì§„ë‹¨ì¥ë¹„":0.9,"ì˜ë£Œì˜ìƒ":0.9,"ë³´ê±´ì˜ë£Œì •ë³´":1.0,"ì œì•½ì‚¬ í•´í‚¹":0.9,
        "ì—°êµ¬ë°ì´í„° ìœ ì¶œ":0.9,"ì„ìƒì‹œí—˜ ë°ì´í„°":0.9,"DNA ë°ì´í„°":0.9,"ë°”ì´ì˜¤í•´í‚¹":0.9,
        "ì•…ì„±ì½”ë“œ":1.0,"ëœì„¬ì›¨ì–´":1.0,"íŠ¸ë¡œì´ëª©ë§ˆ":0.9,"RAT":0.9,"ë£¨íŠ¸í‚·":0.9,
        "í”¼ì‹±":1.0,"ìŠ¤í”¼ì–´í”¼ì‹±":1.0,"ì‚¬íšŒê³µí•™":0.9,"QR í”¼ì‹±":0.8,"ìŠ¤ë¯¸ì‹±":0.8,
        "ì œë¡œë°ì´":1.0,"ì·¨ì•½ì ":1.0,"CVE":1.0,"SQL ì¸ì ì…˜":0.8,"XSS":0.8,"CSRF":0.8,
        "SSRF":0.8,"ì„œë¹„ìŠ¤ ê±°ë¶€":0.9,"DDoS":0.9,"MITM":0.9,"VPN ê³µê²©":0.9,
        "ì˜ë£Œë°ì´í„° ìœ„ì¡°":1.0,"í™˜ì ëª¨ë‹ˆí„°ë§ ì¡°ì‘":1.0,"ì˜ë£Œê¸°ê¸° ì˜¤ì‘ë™":1.0,
        "ë¶ˆë²• ì˜ë£Œ ë°ì´í„° ê±°ë˜":1.0,"ë‹¤í¬ì›¹ ìœ ì¶œ":1.0,"ì•…ì„± ì•±":0.9,"ìœ„ì¡° ì²˜ë°©ì „":1.0,
        "ë³´ì•ˆ ì„¤ì • ë¯¸í¡":0.9,"ì•”í˜¸í™” ë¯¸ì ìš©":1.0,"ì•½í•œ ë¹„ë°€ë²ˆí˜¸":1.0,"2FA ë¯¸ì ìš©":1.0,
        "ì˜ë£Œ AI ìœ„ì¡°":1.0,"í—¬ìŠ¤ì¼€ì–´ IoT ê³µê²©":1.0,"í™˜ì ê³„ì • íƒˆì·¨":1.0,"ì˜ë£Œë³´í—˜ ì‚¬ê¸°":0.9,
        "ì˜ë£Œ ë””ì§€í„¸ íŠ¸ìœˆ í•´í‚¹":0.9,"ì›ê²© ìˆ˜ìˆ  í•´í‚¹":1.0,
        "AI ì§„ë‹¨ ì¡°ì‘":1.0,"í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜":0.8,"AI ì˜ë£Œë°ì´í„° ì¡°ì‘":1.0
    },
    "êµìœ¡ì—…": {
        "ì˜¨ë¼ì¸ìˆ˜ì—…":1.0,"LMS":1.0,"í•™ìƒì •ë³´":1.0,"êµì§ì› ê³„ì •":1.0,"í•™êµ ë„¤íŠ¸ì›Œí¬":0.9,
        "ì—°êµ¬ë°ì´í„°":0.9,"ëŒ€í•™ í•´í‚¹":1.0,"ê³ ë“±í•™êµ í•´í‚¹":0.9,"ì…ì‹œ ë°ì´í„° ìœ ì¶œ":1.0,"ì„±ì  ì¡°ì‘":1.0,
        "í”¼ì‹±":1.0,"ìŠ¤í”¼ì–´í”¼ì‹±":1.0,"ìŠ¤ë¯¸ì‹±":0.9,"QR í”¼ì‹±":0.9,"ì‚¬íšŒê³µí•™":0.9,
        "ëœì„¬ì›¨ì–´":0.9,"ì•…ì„±ì½”ë“œ":0.9,"íŠ¸ë¡œì´ëª©ë§ˆ":0.9,"ì›œ":0.9,"ë°”ì´ëŸ¬ìŠ¤":0.9,"RAT":0.9,
        "ì œë¡œë°ì´":0.9,"ì·¨ì•½ì ":0.9,"SQL ì¸ì ì…˜":0.9,"XSS":0.9,"CSRF":0.9,
        "SSRF":0.9,"ì„œë¹„ìŠ¤ ê±°ë¶€":0.9,"DDoS":0.9,"MITM":0.9,"VPN ê³µê²©":0.9,
        "ë°ì´í„° ìœ ì¶œ":1.0,"ê°œì¸ì •ë³´ ìœ ì¶œ":1.0,"ì¶œì„ ì¡°ì‘":0.9,"ì‹œí—˜ ë¬¸ì œ ìœ ì¶œ":1.0,
        "í•´í‚¹ ë™ì•„ë¦¬":0.7,"ë‹¤í¬ì›¹ ê³µìœ ":0.9,"í¬ë¦¬ë´ì…œ ìŠ¤í„°í•‘":1.0,"ë¸Œë£¨íŠ¸í¬ìŠ¤":0.9,"ì•½í•œ ì•”í˜¸":1.0,
        "ì›ê²© ìˆ˜ì—… íˆ´ ê³µê²©":1.0,"êµìˆ˜ ê³„ì • íƒˆì·¨":1.0,"í•™ìƒ ê³„ì • ë„ìš©":1.0,"êµìœ¡ í´ë¼ìš°ë“œ ì·¨ì•½ì ":0.9,
        "ì˜¨ë¼ì¸ ì‹œí—˜ ë¶€ì •í–‰ìœ„ íˆ´":0.9,
        "AI ìˆ™ì œ ìë™í™”":0.8,"í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜":0.7,"AI ì»¤ë‹ íˆ´":0.8
    },
    "ê¸°íƒ€": {
        "APT":1.0,"ë¼ìë£¨ìŠ¤":1.0,"ê¹€ìˆ˜í‚¤":1.0,"ìƒŒë“œì›œ":1.0,"APT28":1.0,"APT29":1.0,
        "êµ­ê°€ ì§€ì› í•´í‚¹":1.0,"ì‚¬ì´ë²„ì „":1.0,"ì‚¬ì´ë²„ ìŠ¤íŒŒì´":1.0,"ìŠ¤íŒŒì´ì›¨ì–´":1.0,
        "ì‚¬íšŒê³µí•™":1.0,"ì •ì¹˜ ì„ ì „ í•´í‚¹":1.0,"ì •ë¶€ê¸°ê´€ ê³µê²©":1.0,"êµ°ì‚¬ í•´í‚¹":1.0,
        "DDoS":1.0,"ì„œë¹„ìŠ¤ ê±°ë¶€":1.0,"ë°ì´í„° ìœ ì¶œ":1.0,"ê¸°ë°€ ë¬¸ì„œ ìœ ì¶œ":1.0,
        "ëœì„¬ì›¨ì–´":1.0,"ì œë¡œë°ì´":1.0,"ì·¨ì•½ì ":1.0,"ì•…ì„±ì½”ë“œ":1.0,"ë°±ë„ì–´":1.0,
        "ìŠ¤í”¼ì–´í”¼ì‹±":1.0,"BEC":1.0,"ê³µê¸‰ë§ ê³µê²©":1.0,"ì†Œì…œë¯¸ë””ì–´ í•´í‚¹":0.9,"ë””ë„ìŠ¤":1.0,
        "ì„ ê±° í•´í‚¹":1.0,"ì–¸ë¡  ì¡°ì‘":1.0,"ì¸í”„ë¼ ê³µê²©":1.0,"ì „ë ¥ë§ ê³µê²©":1.0,"ìˆ˜ë„ì‹œì„¤ ê³µê²©":1.0,
        "êµí†µë§ í•´í‚¹":1.0,"ìœ„ì„±í†µì‹  í•´í‚¹":1.0,"GPS êµë€":1.0,"IoT ê³µê²©":0.9,
        "ë”¥í˜ì´í¬":1.0,"AI ê¸°ë°˜ ê³µê²©":1.0,"ì•…ì„± ë“œë¡ ":0.9,"ì‚¬ì´ë²„ í…ŒëŸ¬":1.0,"í•µì‹¬ì¸í”„ë¼ íŒŒê´´":1.0,
        "MITRE ATT&CK TTP":1.0,"ì‚¬íšŒ í˜¼ë€ ì¡°ì¥":1.0,"ì‚¬ì´ë²„ ì²©ë³´":1.0,
        "ìš°í¬ë¼ì´ë‚˜ ì „ìŸ í•´í‚¹":1.0,"ì¤‘ë™ ì‚¬ì´ë²„ì „":1.0,"ì‚¬ì´ë²„ ìš©ë³‘":0.9,"ì •ë³´ì „":1.0,
        "AI ì‹¬ë¦¬ì „":1.0,"AI ì„ ì „ ì¡°ì‘":1.0,"AI ê¸°ë°˜ ì—¬ë¡  ì¡°ì‘":1.0,"í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜":0.9
    }
}

def ner_inference(sentence: str):
    """NER ê¸°ë°˜ í† í°â†’ì›Œë“œ ì¬êµ¬ì„± í›„ ë¼ë²¨ O ì œì™¸ í† í° ë°˜í™˜"""
    if not (ner_tokenizer and ner_model and ner_ctx):
        return []
    device, id2label = ner_ctx
    enc = ner_tokenizer(sentence, return_offsets_mapping=True,
                         return_tensors="pt", truncation=True)
    word_ids = enc.word_ids()
    with torch.no_grad():
        outputs = ner_model(input_ids=enc["input_ids"].to(device),
                            attention_mask=enc["attention_mask"].to(device))
        pred_ids = torch.argmax(outputs.logits, dim=-1)[0].cpu().tolist()

    results, cur_word, cur_label, prev_wid = [], "", None, None
    for idx, wid in enumerate(word_ids):
        if wid is None:
            continue
        tok_id = int(enc["input_ids"][0, idx].item())
        piece = ner_tokenizer.convert_ids_to_tokens(tok_id)
        label = ner_ctx[1][int(pred_ids[idx])]
        if wid != prev_wid:
            if cur_word:
                results.append((cur_word, cur_label))
            cur_word, cur_label = piece.replace("##", ""), label
        else:
            cur_word += piece.replace("##", "")
        prev_wid = wid
    if cur_word:
        results.append((cur_word, cur_label))
    return [w for w, l in results if l != "O"]

def classify_cve_industry(description: str) -> str:
    desc = description.lower()
    if any(x in desc for x in ["ics","scada","plc","ot","industrial","factory","hmi"]):
        return "ì œì¡°ì—…"
    elif any(x in desc for x in ["bank","finance","payment","atm","credential","card"]):
        return "ê¸ˆìœµì—…"
    elif any(x in desc for x in ["medical","healthcare","hospital","ehr","emr","patient"]):
        return "ì˜ë£Œì—…"
    elif any(x in desc for x in ["school","student","education","lms","university"]):
        return "êµìœ¡ì—…"
    else:
        return "IT/ì†Œí”„íŠ¸ì›¨ì–´"

def update_keywords_from_cisa(industry_map: dict):
    try:
        kev_url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
        kev_data = requests.get(kev_url, timeout=10).json()
        for vuln in kev_data.get("vulnerabilities", []):
            cve_id = vuln.get("cveID")
            desc = vuln.get("shortDescription", "")
            industry = classify_cve_industry(desc)
            if cve_id and cve_id not in industry_map[industry]:
                industry_map[industry][cve_id] = 1.0
    except Exception as e:
        st.warning(f"CISA KEV ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

# ì•± ìµœì´ˆ 1íšŒ KEV ë°˜ì˜
update_keywords_from_cisa(industry_risk_map)

def analyze_risk_with_model(text: str, industry_type: str):
    """
    1) ê°€ëŠ¥í•˜ë©´ NERë¡œ ì—”í„°í‹° ì¶”ì¶œ
    2) ì—…ì¢…ë³„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ í•©ì‚°ìœ¼ë¡œ ì ìˆ˜/ë ˆë²¨ ì‚°ì •
    3) NER ì‹¤íŒ¨ ì‹œ, ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ í´ë°±
    """
    risk_dict = industry_risk_map.get(industry_type, {})
    extracted = ner_inference(text)

    # í´ë°±: ê´€ì‹¬ ë§µ í‚¤ ì¤‘ í…ìŠ¤íŠ¸ í¬í•¨ë˜ëŠ” ê²ƒ ì¶”ê°€
    if not extracted:
        for kw in risk_dict.keys():
            try:
                if re.search(r'\b' + re.escape(kw) + r'\b', text, flags=re.IGNORECASE):
                    extracted.append(kw)
            except re.error:
                # ì •ê·œì‹ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì œ ëŒ€ë¹„
                if kw.lower() in text.lower():
                    extracted.append(kw)

    extracted = list(set(extracted))
    total_score = sum(risk_dict.get(kw, 0.0) for kw in extracted)
    if total_score >= 2.0: level = "ë†’ìŒ"
    elif total_score >= 0.8: level = "ì¤‘ê°„"
    else: level = "ë‚®ìŒ"
    return level, extracted, total_score

# ============================================================
# 4) LLM ìš”ì•½/í”Œë ˆì´ë¶/í‚¤ì›Œë“œ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë³´ê°•)
# ============================================================
@st.cache_data(show_spinner=False)
def generate_article_summary(title: str, content: str, severity_label: str, company_info: dict, infrastructure: str):
    """
    message.txt ì˜ë„ ë°˜ì˜:
    - 3~5ë¬¸ì¥ ìš”ì•½
    - ë§ˆì§€ë§‰ ì¤„: 'ì™œ ìš°ë¦¬ì—ê²Œ ì¤‘ìš”í•œê°€' 1ë¬¸ì¥
    - ë§¥ë½ íŒíŠ¸: ì‹¬ê°ë„/ì—…ì¢…/ì¸í”„ë¼
    """
    prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ ë³´ì•ˆ ë‰´ìŠ¤ë¥¼ 3~5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
ë§ˆì§€ë§‰ ì¤„ì— 'ì™œ ìš°ë¦¬ì—ê²Œ ì¤‘ìš”í•œê°€'ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
ê³¼ì¥ ì—†ì´ ì‚¬ì‹¤ë§Œ, ê°€ëŠ¥í•˜ë©´ ìˆ˜ì¹˜/ê¸°ìˆ  ìš”ì†Œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ.

[ê¸°ì‚¬ ì œëª©]
{title}

[ê¸°ì‚¬ ë³¸ë¬¸(ë°œì·Œ)]
{content[:2800]}

[ë§¥ë½ íŒíŠ¸]
- ì‹¬ê°ë„: {severity_label}
- ì—…ì¢…: {company_info.get('industry')}
- ì¸í”„ë¼: {infrastructure}

ì¶œë ¥ í˜•ì‹: ë¬¸ë‹¨ 3~5ê°œ + ë§ˆì§€ë§‰ 1ë¬¸ì¥(ì™œ ì¤‘ìš”í•œê°€).
""".strip()
    try:
        res = GEMINI.generate_content(prompt)
        return res.text
    except Exception:
        return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨."

def generate_playbook_with_llm(keywords, company_info, infrastructure, constraints, news_briefs=None):
    """
    - message.txt ì˜ë„ ë°˜ì˜ í†µí•© í”Œë ˆì´ë¶:
      ì¦‰ì‹œ/7ì¼/30ì¼ êµ¬ê°„ + íƒì§€ë£° + ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ + ì²´í¬ë¦¬ìŠ¤íŠ¸
    - LLM ì¸í’‹ ë° ê²°ê³¼ ë¡œê·¸ ì €ì¥
    - ì¤‘ìš” í‚¤ì›Œë“œ JSON ì¬ìš”ì²­
    """
    # 0) ìƒìœ„ ë‰´ìŠ¤ 1ì¤„ ìš”ì•½ ëª©ë¡
    news_briefs = news_briefs or []
    company_info_str = json.dumps(company_info, ensure_ascii=False)

    mode_line = "ê°€ëŠ¥í•œ ì €ì˜ˆì‚°/ê°„ì†Œí™” ëª¨ë“œë¥¼ ìš°ì„  ê³ ë ¤" if (constraints and any(x in constraints.lower() for x in ["ì €ì˜ˆì‚°","budget","ë¹„ìš©","í•œì •"])) else "í‘œì¤€ ëª¨ë“œë¡œ ì‹¤í–‰"
    
    # 1) ë³¸ë¬¸ í”Œë ˆì´ë¶ ìƒì„± í”„ë¡¬í”„íŠ¸
    prompt = f"""
ë‹¹ì‹ ì€ ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **í†µí•© ì¥ë¬¸ ëŒ€ì‘ í”Œë ˆì´ë¶**ì„ ì‘ì„±í•˜ì„¸ìš”.
- ì¤‘ë³µë˜ëŠ” ì¡°ì¹˜ëŠ” í†µí•©/ì •ë¦¬
- **ì¦‰ì‹œ(ì˜¤ëŠ˜~48h)/7ì¼/30ì¼** êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆŒ ê²ƒ
- {mode_line}
- **êµ¬ì²´ ì„¤ì •/ì„œë¹„ìŠ¤ëª…**(ì˜ˆ: AWS/GCP/Azure, EDR/SIEM/MFA ë“±)ì„ í¬í•¨
- ê° ì¡°ì¹˜ëŠ” **ê²€ì¦ ê¸°ì¤€(ì–´ë–»ê²Œ í™•ì¸í• ì§€)**ì„ ëª…ì‹œ

[íšŒì‚¬ í”„ë¡œí•„(JSON)]
{company_info_str}

[ì¸í”„ë¼]
{infrastructure}

[ìµœì‹  ë³´ì•ˆ í‚¤ì›Œë“œ í›„ë³´]
{", ".join(keywords[:40])}

[ìƒìœ„ ë‰´ìŠ¤ ìš”ì•½(ê° 1ì¤„)]
{chr(10).join(f"- {line}" for line in news_briefs[:8]) if news_briefs else "- (ì—†ìŒ)"}

[ì œì•½]
{constraints or "ì—†ìŒ"}

ì¶œë ¥ í˜•ì‹: Markdown ì„¹ì…˜
1) ìƒí™©ìš”ì•½
2) ì¦‰ì‹œ(ì˜¤ëŠ˜~48h)
3) 7ì¼
4) 30ì¼
5) íƒì§€ë£°/ëª¨ë‹ˆí„°ë§(ë¡œê·¸ ì†ŒìŠ¤, ë£° ë˜ëŠ” ì¿¼ë¦¬ ê°œìš”)
6) ì»¤ë®¤ë‹ˆì¼€ì´ì…˜(ì„ì§ì› ê³µì§€/í›ˆë ¨/ì™¸ë¶€ ë³´ê³ )
7) ì²´í¬ë¦¬ìŠ¤íŠ¸(ì¸¡ì • ê°€ëŠ¥í•œ ì™„ë£Œ ì¡°ê±´)
""".strip()

    try:
        resp = GEMINI.generate_content(prompt)
        playbook = resp.text or ""
    except Exception as e:
        playbook = f"í”Œë ˆì´ë¶ ìƒì„± ì‹¤íŒ¨: {e}"

    # 2) LLMì´ ì¤‘ìš”í•˜ë‹¤ê³  íŒë‹¨í•œ í‚¤ì›Œë“œë§Œ JSONìœ¼ë¡œ ì¬ìš”ì²­
    kw_prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œ í›„ë³´ì—ì„œ ì¤‘ì†Œê¸°ì—… í™˜ê²½ì— ê°€ì¥ ê´€ë ¨ ë†’ì€ ìƒìœ„ 12ê°œë¥¼ ê³ ë¥´ì„¸ìš”.
JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

í›„ë³´: {', '.join(keywords)}

ìŠ¤í‚¤ë§ˆ:
[
  {{"keyword": "ë¬¸ìì—´", "rationale": "ê°„ë‹¨ ê·¼ê±°(10ì~30ì)"}}
]
ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
""".strip()
    llm_selected_keywords = []
    try:
        kw_resp = GEMINI.generate_content(kw_prompt)
        raw = (kw_resp.text or "").strip()
        # JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ìš”ì²­í–ˆì§€ë§Œ ë°©ì–´ì ìœ¼ë¡œ íŒŒì‹±
        json_str = re.search(r'\[.*\]', raw, flags=re.S)
        if json_str:
            llm_selected_keywords = json.loads(json_str.group(0))
        else:
            raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨")
    except Exception:
        # ì‹¤íŒ¨ ì‹œ ìƒìœ„ 12ê°œ í‚¤ì›Œë“œ ë‹¨ìˆœ ì ˆë‹¨
        llm_selected_keywords = [{"keyword": k, "rationale": "ìë™ ëŒ€ì²´(íŒŒì‹± ì‹¤íŒ¨)"} for k in keywords[:12]]

    st.session_state.llm_selected_keywords = llm_selected_keywords
    return playbook

# ============================================================
# 5) PDF ë³´ê³ ì„œ (í°íŠ¸ ê²½ë¡œ ìœ ì—°í™” + NanumGothic ì ìš© + ì•ˆì „ ë˜í•‘)
# ============================================================
import textwrap
import re
from fpdf import FPDF

def _try_add_font(pdf: FPDF):
    # ê°€ëŠ¥í•œ ê²½ë¡œë“¤: ë¡œì»¬/ìƒëŒ€ê²½ë¡œ ëª¨ë‘ ì‹œë„
    candidates = [
        "font/NanumGothic.ttf",
        "font/NanumGothicBold.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "C:/Windows/Fonts/NanumGothic.ttf",
        "C:/Windows/Fonts/NanumGothicBold.ttf",
    ]
    for path in candidates:
        try:
            # ë™ì¼ íŒ¨ë°€ë¦¬ëª…ìœ¼ë¡œ í•˜ë‚˜ë§Œ ë“±ë¡í•´ë„ ë³¸ë¬¸ ì‚¬ìš©ì—ëŠ” ì§€ì¥ ì—†ìŒ
            pdf.add_font("Nanum", "", path, uni=True)
            return True
        except Exception:
            continue
    return False

def _usable_width(pdf: FPDF) -> float:
    # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì¢Œ/ìš° ë§ˆì§„ì„ ì œì™¸í•œ ì‚¬ìš© ê°€ëŠ¥ í­
    return pdf.w - pdf.l_margin - pdf.r_margin

def _normalize_long_tokens(s: str) -> str:
    """
    FPDFê°€ ì¤„ë°”ê¿ˆí•˜ì§€ ëª»í•˜ëŠ” ê¸´ í† í°(URL, CVE, ê²½ë¡œ ë“±)ì„ ì•ˆì „í•˜ê²Œ ëŠê¸° ìœ„í•´
    ë¶„ë¦¬ì ë’¤ì— ì—¬ë°±ì„ ì¶”ê°€í•´ ê°€ì‹œì  ëŠê¹€ í¬ì¸íŠ¸ë¥¼ ë§Œë“ ë‹¤.
    """
    # ë¶„ë¦¬ì ë’¤ì— ê³µë°± ì¶”ê°€
    s = re.sub(r'([/@:_\-\.\|\+\=])', r'\1 ', s)
    # ë‹¤ì¤‘ ê³µë°± ì¶•ì†Œ
    s = re.sub(r'\s{2,}', ' ', s)
    return s

def _safe_multicell(pdf: FPDF, text: str, line_height: float = 7.0, width: float = None, wrap_chars: int = 100):
    """
    - í­ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ ë‚¨ì€ í­ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ìƒí™© ë°©ì§€
    - ê¸´ í† í°ì„ ì •ê·œí™”í•˜ê³ , ì‹¤íŒ¨ ì‹œ ê¸€ììˆ˜ ê¸°ì¤€ í•˜ë“œ ë˜í•‘ìœ¼ë¡œ í´ë°±
    """
    if width is None:
        width = _usable_width(pdf)

    # í•­ìƒ ì¢Œì¸¡ ë§ˆì§„ìœ¼ë¡œ ìœ„ì¹˜ ì´ˆê¸°í™”
    pdf.set_x(pdf.l_margin)

    # 1ì°¨: ì •ìƒ ì¶œë ¥ ì‹œë„
    try:
        norm = _normalize_long_tokens(text)
        # textwrapìœ¼ë¡œ 1ì°¨ ë˜í•‘(í•œ ì¤„ ìµœëŒ€ ê¸€ììˆ˜ ê¸°ì¤€)
        wrapped = textwrap.fill(norm, width=wrap_chars)
        pdf.multi_cell(width, line_height, wrapped)
        return
    except Exception:
        pass

    # 2ì°¨: í°íŠ¸ í¬ê¸° 1pt ë‚®ì¶° ì¬ì‹œë„
    cur_family, cur_style, cur_size = pdf.font_family, pdf.font_style, pdf.font_size_pt
    try:
        if cur_size > 8:
            pdf.set_font(cur_family, cur_style, cur_size - 1)
        pdf.set_x(pdf.l_margin)
        wrapped = textwrap.fill(_normalize_long_tokens(text), width=max(60, wrap_chars - 20))
        pdf.multi_cell(width, line_height, wrapped)
        return
    except Exception:
        pass
    finally:
        # í°íŠ¸ ë³µêµ¬
        pdf.set_font(cur_family, cur_style, cur_size)

    # 3ì°¨: í•˜ë“œ ìŠ¬ë¼ì´ìŠ¤(ê°•ì œ ì²­í¬ ë¶„í• )
    for i in range(0, len(text), 100):
        pdf.set_x(pdf.l_margin)
        chunk = text[i:i+100]
        pdf.multi_cell(width, line_height, chunk)

def create_pdf_report(report_data, company_name="ì¤‘ì†Œê¸°ì—…"):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()

    # í°íŠ¸ ì„¤ì •
    if _try_add_font(pdf):
        base_font = "Nanum"
        title_size = 20
        h1_size = 14
        body_size = 12
    else:
        # fallback (ì˜ë¬¸ ì „ìš©)
        base_font = "Arial"
        title_size = 16
        h1_size = 13
        body_size = 11

    # ì œëª©
    pdf.set_font(base_font, "", title_size)
    pdf.set_x(pdf.l_margin)
    pdf.cell(_usable_width(pdf), 10, f"{company_name} ë³´ì•ˆ ë¶„ì„ ë³´ê³ ì„œ", 0, 1, "C")
    pdf.ln(6)

    # 1. ìš”ì•½
    pdf.set_font(base_font, "", h1_size)
    pdf.set_x(pdf.l_margin)
    pdf.cell(_usable_width(pdf), 10, "1. ìš”ì•½ ì •ë³´", 0, 1)
    pdf.set_font(base_font, "", body_size)
    _safe_multicell(pdf, report_data.get("summary", ""), line_height=7.0, width=_usable_width(pdf), wrap_chars=100)

    # 2. í‚¤ì›Œë“œ
    pdf.ln(5)
    pdf.set_font(base_font, "", h1_size)
    pdf.set_x(pdf.l_margin)
    pdf.cell(_usable_width(pdf), 10, "2. ì£¼ìš” í‚¤ì›Œë“œ", 0, 1)
    pdf.set_font(base_font, "", body_size)
    for kw in report_data.get("keywords", []):
        keyword = str(kw.get("keyword", ""))
        level = kw.get("risk_level") or kw.get("interest_level") or ""
        freq  = kw.get("frequency", "")
        line = f"- {keyword} | ë ˆë²¨: {level} | ë¹ˆë„: {freq}"
        _safe_multicell(pdf, line, line_height=7.0, width=_usable_width(pdf), wrap_chars=80)

    # 3. ëŒ€ì‘ í”Œë ˆì´ë¶
    pdf.ln(5)
    pdf.set_font(base_font, "", h1_size)
    pdf.set_x(pdf.l_margin)
    pdf.cell(_usable_width(pdf), 10, "3. AI ìƒì„± ëŒ€ì‘ í”Œë ˆì´ë¶", 0, 1)
    pdf.set_font(base_font, "", body_size)
    _safe_multicell(pdf, report_data.get("playbook", ""), line_height=7.0, width=_usable_width(pdf), wrap_chars=100)

    # ë°”ì´íŠ¸ ë°˜í™˜ (Streamlit download_buttonì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)
    out = pdf.output(dest="S")
    if isinstance(out, str):
        out = out.encode("latin1", errors="ignore")
    return bytes(out)


# ============================================================
# 6) ì‚¬ì´ë“œë°” (í™˜ê²½ ì„¤ì • / RSS ì „ìš©)
# ============================================================
with st.sidebar:
    st.markdown("""
    <div class="sidebar-logo">
        <h2>ğŸ›¡ï¸ SecureWatch</h2>
        <p>ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ë¶„ì„</p>
    </div>
    """, unsafe_allow_html=True)

    st.subheader("ğŸ¢ ê¸°ì—… ì •ë³´ ì„¤ì •")
    company_size = st.selectbox("ê¸°ì—… ê·œëª¨", ["ì†Œê·œëª¨ (10-50ëª…)", "ì¤‘ì†Œê·œëª¨ (50-200ëª…)", "ì¤‘ê·œëª¨ (200-500ëª…)"])
    industry_type = st.selectbox("ì—…ì¢…", ["IT/ì†Œí”„íŠ¸ì›¨ì–´", "ì œì¡°ì—…", "ê¸ˆìœµì—…", "ì˜ë£Œì—…", "êµìœ¡ì—…", "ê¸°íƒ€"])

    st.subheader("ğŸŒ ì¸í”„ë¼ ë° ì œì•½ì‚¬í•­")
    infrastructure = st.selectbox("ì¸í”„ë¼ í™˜ê²½", ["AWS", "Azure", "GCP", "On-premise", "Hybrid"])
    constraints = st.text_area("ë³´ì•ˆ ì •ì±…/ì˜ˆì‚° ë“± ì œí•œì‚¬í•­", value="")
    user_interest = st.text_area("ê´€ì‹¬ ë¶„ì•¼ í‚¤ì›Œë“œ(ì‰¼í‘œ êµ¬ë¶„)", value="")
    st.session_state.user_interest = user_interest

    st.divider()
    if st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary"):
        st.session_state.analysis_started = True
        st.session_state.news_data = []
        st.session_state.risk_keywords = []
        st.session_state.playbook_content = ""
        st.session_state.report_summary = ""
        st.session_state.llm_selected_keywords = []

        with st.spinner("RSSì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            articles = fetch_latest_news_by_rss()

        # ë¶„ì„
        with st.spinner("ë¶„ì„/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."):
            news_data = []
            keyword_counts = {}
            for art in articles:
                combined = f"{art['title']} {art['content']}"
                risk_level, kws, score = analyze_risk_with_model(combined, industry_type)
                for k in kws:
                    keyword_counts[k] = keyword_counts.get(k, 0) + 1
                news_data.append({
                    "title": art['title'],
                    "summary": art['content'][:250] + "..." if len(art['content']) > 250 else art['content'],
                    "full_content": art['content'],
                    "source": art.get('source', 'ë³´ì•ˆë‰´ìŠ¤'),
                    "published": art.get('date', ''),
                    "risk_level": risk_level,
                    "risk_score": score,
                    "keywords": kws,
                    "url": art['url'],
                    "summary_llm": "" # LLM ìš”ì•½ ê²°ê³¼ë¥¼ ì €ì¥í•  í•„ë“œ ì¶”ê°€
                })
            user_interest_list = [kw.strip() for kw in user_interest.split(',') if kw.strip()]
            for uk in user_interest_list:
                keyword_counts[uk] = keyword_counts.get(uk, 0) + 1
            st.session_state.news_data = sorted(news_data, key=lambda x: x['risk_score'], reverse=True)
            st.session_state.risk_keywords = [
                {"keyword": kw, "frequency": cnt, "risk_level": analyze_risk_with_model(kw, industry_type)[0]}
                for kw, cnt in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
            ]

        # ìƒìœ„ ë‰´ìŠ¤ 1ì¤„ ìš”ì•½(í”Œë ˆì´ë¶ì— ì „ë‹¬í•  ë¸Œë¦¬í”„)
        with st.spinner("ìƒìœ„ ë‰´ìŠ¤ ìš”ì•½ ì •ë¦¬ ì¤‘..."):
            top_for_brief = st.session_state.news_data[:6]
            news_briefs = []
            for n in top_for_brief:
                brief = f"{n['title']} | í‚¤ì›Œë“œ: {', '.join(n['keywords'][:3])} | ê´€ì‹¬ë„ {n['risk_level']}"
                news_briefs.append(brief)

        with st.spinner("LLM í”Œë ˆì´ë¶ ìƒì„± ì¤‘..."):
            company_name = "ì¤‘ì†Œê¸°ì—…"
            company_info = {"name": company_name, "size": company_size, "industry": industry_type}
            keywords_list = [k["keyword"] for k in st.session_state.risk_keywords]
            st.session_state.playbook_content = generate_playbook_with_llm(
                keywords_list, company_info, infrastructure, constraints, news_briefs=news_briefs
            )

        st.session_state.report_summary = f"ì´ {len(st.session_state.news_data)}ê°œ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ."
        st.success("âœ… ë¶„ì„ ì™„ë£Œ! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.rerun()

# ============================================================
# 7) ë©”ì¸ í—¤ë”
# ============================================================
st.markdown("""
<div class="main-header">
    <h1>ğŸ›¡ï¸ ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ë¶„ì„ ì‹œìŠ¤í…œ</h1>
    <p>AI ê¸°ë°˜ ë³´ì•ˆ ìœ„í˜‘ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë§ì¶¤í˜• ëŒ€ì‘ í”Œë ˆì´ë¶ ìƒì„±</p>
</div>
""", unsafe_allow_html=True)

# ============================================================
# 8) íƒ­: ëŒ€ì‹œë³´ë“œ/ë‰´ìŠ¤/í”Œë ˆì´ë¶
# ============================================================
tab1, tab2, tab3 = st.tabs([
    "ğŸ“Š ëŒ€ì‹œë³´ë“œ", "ğŸ“° ë‰´ìŠ¤ ë¶„ì„", "ğŸ“‹ ëŒ€ì‘ í”Œë ˆì´ë¶"
])

# --- ëŒ€ì‹œë³´ë“œ
with tab1:
    st.header("ğŸ“Š ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ëŒ€ì‹œë³´ë“œ")
    if 'analysis_started' not in st.session_state:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ì—… ì •ë³´ë¥¼ ì„¤ì •í•˜ê³  'ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        top_news = st.session_state.news_data[:2]
        if not top_news:
            st.info("ë¶„ì„ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for current in top_news:
                css_class = "risk-high" if current["risk_level"] == "ë†’ìŒ" else "risk-medium" if current["risk_level"] == "ì¤‘ê°„" else "risk-low"
                
                # ëŒ€ì‹œë³´ë“œì—ì„œ ìš”ì•½ ê¸°ëŠ¥ ì œê±° (ëŒ€ì‹  ë‰´ìŠ¤ ë¶„ì„ íƒ­ì—ì„œ ì œê³µ)
                summary_content = current['summary'] + " (ë” ìì„¸í•œ ìš”ì•½ì€ 'ë‰´ìŠ¤ ë¶„ì„' íƒ­ì—ì„œ í™•ì¸í•˜ì„¸ìš”.)"
    
                st.markdown(f"""
                <div class="news-item {css_class}">
                    <div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:1rem;">
                        <h5 style="margin:0;color:#2c3e50;">
                            <a href="{current['url']}" target="_blank">{current['title']}</a>
                        </h5>
                        <span style="background:{'#e74c3c' if current['risk_level']=='ë†’ìŒ' else '#f39c12' if current['risk_level']=='ì¤‘ê°„' else '#27ae60'};color:white;padding:0.3rem 0.8rem;border-radius:15px;font-size:0.8rem;font-weight:bold;white-space:nowrap;">
                            ê´€ì‹¬ë„: {current['risk_level']} ({current['risk_score']:.2f})
                        </span>
                    </div>
                    <p style="color:#555; margin-bottom:1rem; white-space: pre-wrap;">{summary_content}</p>
                    <div style="color:#888; font-size:0.9rem;">
                        <strong>í‚¤ì›Œë“œ:</strong> {', '.join(current['keywords'])} | {current['source']} | {current['published']}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                st.markdown("---") # í•­ëª© ê°„ êµ¬ë¶„ì„ 

# --- ë‰´ìŠ¤ ë¶„ì„
with tab2:
    st.header("ğŸ“° ìµœì‹  ë³´ì•ˆ ë‰´ìŠ¤ ë¶„ì„")
    if 'analysis_started' not in st.session_state:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ì—… ì •ë³´ë¥¼ ì„¤ì •í•˜ê³  'ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        # ê´€ì‹¬ í‚¤ì›Œë“œ ìš°ì„  ì •ë ¬ ì œê±°, ê¸°ë³¸ ìœ„í—˜ë„ ìˆœ ì •ë ¬ ìœ ì§€
        sorted_news = st.session_state.news_data
        
        page_size = 10
        total = len(sorted_news)
        total_pages = (total - 1) // page_size + 1 if total > 0 else 1
        if 'current_page' not in st.session_state:
            st.session_state.current_page = 1
        start = (st.session_state.current_page - 1) * page_size
        end = start + page_size
        page_data = sorted_news[start:end]
        
        for idx, news in enumerate(page_data):
            css_class = "risk-high" if news["risk_level"] == "ë†’ìŒ" \
                else "risk-medium" if news["risk_level"] == "ì¤‘ê°„" else "risk-low"
            
            # í•˜ë‚˜ì˜ markdown ë¸”ë¡ìœ¼ë¡œ ì „ì²´ ë‰´ìŠ¤ í•­ëª© í‘œì‹œ
            st.markdown(f"""
            <div class="news-item {css_class}">
                <div style="display:flex;justify-content:space-between;align-items:center;margin-bottom:1rem;">
                    <h5 style="margin:0;color:#2c3e50;">
                        <a href="{news['url']}" target="_blank">{news['title']}</a>
                    </h5>
                    <span style="background:{'#e74c3c' if news['risk_level']=='ë†’ìŒ' else '#f39c12' if news['risk_level']=='ì¤‘ê°„' else '#27ae60'};color:white;padding:0.3rem 0.8rem;border-radius:15px;font-size:0.8rem;font-weight:bold;white-space:nowrap;">
                        ê´€ì‹¬ë„: {news['risk_level']} ({news['risk_score']:.2f})
                    </span>
                </div>
                <p style="color:#555; margin-bottom:1rem; white-space: pre-wrap;">{news['summary']}</p>
                <div style="color:#888; font-size:0.9rem;">
                    <strong>í‚¤ì›Œë“œ:</strong> {', '.join(news['keywords'])} | {news['source']} | {news['published']}
                </div>
            </div>
            """, unsafe_allow_html=True)
            
        st.divider()
        c1, c2, c3, c4, c5 = st.columns(5)
        with c1:
            if st.button("<< ì²˜ìŒ", key='p_first'): st.session_state.current_page = 1
        with c2:
            if st.button("< ì´ì „", key='p_prev', disabled=st.session_state.current_page == 1): st.session_state.current_page -= 1
        with c4:
            if st.button("ë‹¤ìŒ >", key='p_next', disabled=st.session_state.current_page == total_pages): st.session_state.current_page += 1
        with c5:
            if st.button("ë§ˆì§€ë§‰ >>", key='p_last'): st.session_state.current_page = total_pages
        with c3:
            st.markdown(f"<p style='text-align:center; font-weight:bold;'>{st.session_state.current_page} / {total_pages}</p>", unsafe_allow_html=True)

# --- ëŒ€ì‘ í”Œë ˆì´ë¶
with tab3:
    if 'analysis_started' not in st.session_state:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        # PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í—¤ë” ì˜†ì— ë°°ì¹˜
        col1, col2 = st.columns([0.7, 0.3])
        with col1:
            st.markdown("### ğŸ“‹ AI ìƒì„± ëŒ€ì‘ í”Œë ˆì´ë¶")
        with col2:
            st.markdown('<div style="height: 1.5rem;"></div>', unsafe_allow_html=True) # í—¤ë”ì™€ ë†’ì´ ë§ì¶”ê¸°
            report_data = {
                "summary": st.session_state.report_summary,
                "keywords": st.session_state.risk_keywords,
                "playbook": st.session_state.playbook_content
            }
            pdf_output = create_pdf_report(report_data, "ì¤‘ì†Œê¸°ì—…")
            st.download_button(
                label="ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ",
                data=pdf_output,
                file_name=f"ë³´ì•ˆ_ë¶„ì„_ë³´ê³ ì„œ_ì¤‘ì†Œê¸°ì—….pdf",
                mime="application/pdf"
            )

        st.markdown(
            f"""<div class="recommendation-box">
            <p style="white-space: pre-wrap;">{st.session_state.playbook_content}</p></div>""",
            unsafe_allow_html=True
        )
        if st.session_state.llm_selected_keywords:
            st.subheader("ğŸ§© LLMì´ ì„ ë³„í•œ ì¤‘ìš” í‚¤ì›Œë“œ(Top)")
            df_llm_kw = pd.DataFrame(st.session_state.llm_selected_keywords)
            st.dataframe(df_llm_kw, use_container_width=True)

# í‘¸í„°
st.divider()
st.markdown(
    """<div style="text-align:center;color:#888;padding:1rem;">
    <p>ğŸ›¡ï¸ SecureWatch - ì¤‘ì†Œê¸°ì—… ë³´ì•ˆ ê´€ì‹¬/ìœ„í—˜ ë¶„ì„ ì‹œìŠ¤í…œ</p>
    <p>AI ê¸°ë°˜ ìœ„í˜‘ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ëŒ€ì‘ í”Œë ˆì´ë¶ ìƒì„±</p>
    </div>""",
    unsafe_allow_html=True
)