import pandas as pd
import re, ast, random, os
from typing import List, Tuple
from datasets import Dataset, DatasetDict, Features, Sequence, ClassLabel, Value

# ì…ë ¥ ë° ì¶œë ¥ íŒŒì¼/í´ë” ì„¤ì •
# ì²« ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ 'articles_preprocessed.csv' íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
SRC = "articles_preprocessed.csv"
OUT_DIR = "."  # ì¶œë ¥ë¬¼ì„ í˜„ì¬ í´ë”ì— ì €ì¥
HF_DATASET_DIR = os.path.join(OUT_DIR, "ner_dataset")
random.seed(42)

# ---
# NER íƒœê·¸ ë ˆì´ë¸” ì •ì˜
# ---
LABELS = [
    'O',
    'B-ORG', 'I-ORG',
    'B-VULN', 'I-VULN',
    'B-ATTACK', 'I-ATTACK',
    'B-PROD', 'I-PROD',
    'B-EVT', 'I-EVT',
]

# ---
# ë„ìš°ë¯¸ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì „ì²˜ë¦¬2ë‹¨ê³„ íŒŒì¼ì˜ ë‚´ìš©)
# ---
def safe_list(x) -> list:
    """ë¬¸ìì—´ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤."""
    if isinstance(x, list):
        return x
    if pd.isna(x):
        return []
    try:
        # ast.literal_evalë¡œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë¬¸ìì—´ì„ íŒŒì‹±
        v = ast.literal_eval(x)
        if isinstance(v, list):
            # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì— NaNì´ ìˆì„ ê²½ìš° ì œê±°
            return [item for item in v if pd.notna(item) and isinstance(item, str)]
        return []
    except Exception as e:
        # íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        print(f"DEBUG: 'safe_list' ì˜¤ë¥˜ - '{x}' íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

def normalize_list(v: List[str]) -> List[str]:
    """ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ë¬¸ìì—´ì„ ì •ë¦¬í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
    out = []
    for s in v:
        if not isinstance(s, str):
            continue
        s = s.strip()
        if len(s) < 2:
            continue
        out.append(s)
    out = sorted(set(out), key=lambda z: len(z), reverse=True)
    return out

def sentence_split(text: str) -> List[Tuple[str, int]]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    sents = []
    start = 0
    pattern = re.compile(r'(.+?)(?:[\.\?\!]+|\n+|ë‹¤\.)\s*', flags=re.S)
    for m in pattern.finditer(text):
        seg = m.group(0)
        sents.append((seg.strip(), m.start()))
        start = m.end()
    if start < len(text):
        tail = text[start:].strip()
        if tail:
            sents.append((tail, start))
    return sents

def find_entity_spans(text: str, phrases: List[str]) -> List[Tuple[int, int]]:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ê°€ í¬í•¨ëœ ìœ„ì¹˜(span)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    occupied = [False] * len(text)
    spans = []
    for p in phrases:
        if not p or len(p) < 2:
            continue
        use_boundary = bool(re.search(r'[A-Za-z0-9]', p))
        if use_boundary:
            pat = re.compile(rf'\b{re.escape(p)}\b', flags=re.I)
        else:
            pat = re.compile(re.escape(p))
        for m in pat.finditer(text):
            s, e = m.start(), m.end()
            if any(occupied[s:e]):
                continue
            spans.append((s, e))
            for i in range(s, e):
                occupied[i] = True
    return spans

def tokens_with_offsets(sent: str, sent_start: int):
    """ë¬¸ì¥ì„ í† í°í™”í•˜ê³  ì˜¤í”„ì…‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    for m in re.finditer(r'\w+|[^\w\s]', sent, flags=re.UNICODE):
        tok = m.group(0)
        s = sent_start + m.start()
        e = sent_start + m.end()
        yield tok, s, e

def assign_bio(tokens: List[Tuple[str, int, int]], spans_by_type: dict) -> List[Tuple[str, str]]:
    """í† í°ì— BIO íƒœê·¸ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. ìš°ì„ ìˆœìœ„: VULN > ATTACK > PROD > EVT > ORG"""
    prio = ["VULN", "ATTACK", "PROD", "EVT", "ORG"]
    tags = []
    for tok, s, e in tokens:
        tag = "O"
        hit_type = None
        hit_span = None
        for t in prio:
            for (ss, ee) in spans_by_type[t]:
                if not (e <= ss or s >= ee):
                    hit_type = t
                    hit_span = (ss, ee)
                    break
            if hit_type:
                break
        if hit_type:
            if s == hit_span[0]:
                tag = f"B-{hit_type}"
            else:
                tag = f"I-{hit_type}"
        tags.append((tok, tag))
    return tags

def to_dict_format(seqs: List[Tuple[List[str], List[int]]]) -> dict:
    """ë°ì´í„°ë¥¼ HuggingFace Dataset í˜•ì‹ì— ë§ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    tokens = [s[0] for s in seqs]
    labels = [s[1] for s in seqs]
    return {"tokens": tokens, "ner_tags": labels}

# ---
# í›„ì²˜ë¦¬ ëª¨ë“ˆ (ìƒˆë¡œ ì¶”ê°€ëœ ë‚´ìš©)
# ---
def postprocess_ner_results(tokens: List[str], ner_tags_as_numbers: List[int]) -> dict:
    """
    ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼(í† í°ê³¼ ìˆ«ì íƒœê·¸)ë¥¼ ë°›ì•„
    ì›í•˜ëŠ” í˜•ì‹ì˜ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        tokens (List[str]): ì…ë ¥ ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” í† í° ëª©ë¡.
        ner_tags_as_numbers (List[int]): ê° í† í°ì— ëŒ€í•œ ì˜ˆì¸¡ íƒœê·¸(ìˆ«ì) ëª©ë¡.
    
    Returns:
        Dict[str, List[str]]: ë¶„ë¥˜ëœ í‚¤ì›Œë“œë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬.
                              ì˜ˆ: {'ATTACK': ['ëœì„¬ì›¨ì–´ ê³µê²©'], 'VULN': ['ì œë¡œë°ì´ ì·¨ì•½ì ']}
    """
    
    # ê²°ê³¼ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    results = {
        'ORG': [],
        'VULN': [],
        'ATTACK': [],
        'PROD': [],
        'EVT': []
    }
    
    current_entity = ""
    current_tag_type = ""
    
    # ì˜ˆì¸¡ëœ íƒœê·¸ì™€ í† í°ì„ ìˆœíšŒí•˜ë©° í‚¤ì›Œë“œ ì¶”ì¶œ
    for i, (token, tag_number) in enumerate(zip(tokens, ner_tags_as_numbers)):
        tag = LABELS[tag_number]
        
        # B- (Beginning) íƒœê·¸ë¥¼ ë§Œë‚¬ì„ ë•Œ
        if tag.startswith('B-'):
            # ì´ì „ ê°œì²´ëª…ì´ ìˆìœ¼ë©´ ì €ì¥
            if current_entity and current_tag_type:
                results[current_tag_type].append(current_entity.strip())
            
            # ìƒˆë¡œìš´ ê°œì²´ëª… ì‹œì‘
            current_entity = token
            current_tag_type = tag.split('-')[1]
            
        # I- (Inside) íƒœê·¸ë¥¼ ë§Œë‚¬ì„ ë•Œ
        elif tag.startswith('I-'):
            # í˜„ì¬ ê°œì²´ëª…ì— í† í° ì¶”ê°€
            if current_tag_type == tag.split('-')[1]:
                current_entity += " " + token
            else:
                # íƒœê·¸ê°€ ë¶ˆì¼ì¹˜í•˜ë©´ ì´ì „ ê°œì²´ëª… ì €ì¥ í›„ ìƒˆë¡œìš´ ê°œì²´ëª…ìœ¼ë¡œ ì·¨ê¸‰ (ì˜¤ë¥˜ ì²˜ë¦¬)
                if current_entity and current_tag_type:
                    results[current_tag_type].append(current_entity.strip())
                current_entity = ""
                current_tag_type = ""
                
        # O (Outside) íƒœê·¸ë¥¼ ë§Œë‚¬ì„ ë•Œ
        else:
            if current_entity and current_tag_type:
                results[current_tag_type].append(current_entity.strip())
            current_entity = ""
            current_tag_type = ""

    # ë§ˆì§€ë§‰ì— ë‚¨ì€ ê°œì²´ëª… ì €ì¥
    if current_entity and current_tag_type:
        results[current_tag_type].append(current_entity.strip())
        
    return results


# ---
# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ---
if __name__ == "__main__":
    print("---")
    print("ğŸš€ BIO+Dataset ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘!")
    
    # --------------------------
    # 1. ì›ë³¸ ë°ì´í„° ì½ê¸°
    # --------------------------
    print("1. 'articles_preprocessed.csv' íŒŒì¼ ì½ëŠ” ì¤‘...")
    df = pd.read_csv(SRC)
    df = df.fillna('')
    
    # --------------------------
    # 2. BIO íƒœê·¸ ì‹œí€€ìŠ¤ ìƒì„±
    # --------------------------
    print("2. BIO íƒœê·¸ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
    
    # ì´ ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤.
    rows = []
    print(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ì´ í–‰ ê°œìˆ˜: {len(df)}")
    
    for i, r in df.iterrows():
        text = str(r.get("clean_text", "") or "")
        orgs = normalize_list(safe_list(r.get("ORG", [])))
        vulns = normalize_list(safe_list(r.get("VULN", [])))
        attacks = normalize_list(safe_list(r.get("ATTACK", [])))
        prods = normalize_list(safe_list(r.get("PROD", [])))
        evts = normalize_list(safe_list(r.get("EVT", [])))

        if not text.strip():
            continue
        
        sents = sentence_split(text)
        for sent, s0 in sents:
            toks = list(tokens_with_offsets(sent, s0))
            if not toks:
                continue
            spans_by_type = {
                "ORG":      find_entity_spans(text, orgs),
                "VULN":     find_entity_spans(text, vulns),
                "ATTACK":   find_entity_spans(text, attacks),
                "PROD":     find_entity_spans(text, prods),
                "EVT":      find_entity_spans(text, evts)
            }
            seq = assign_bio(toks, spans_by_type)
            if any(lab != "O" for _, lab in seq):
                rows.append(seq)
    
    seqs = rows
    
    # --------------------------
    # 3. ë°ì´í„°ì…‹ ë¶„í•  ë° ì €ì¥
    # --------------------------
    print("3. ë°ì´í„°ì…‹ ë¶„í•  ë° HuggingFace Datasetìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    
    features = Features({
        "tokens": Sequence(Value("string")),
        "ner_tags": Sequence(ClassLabel(names=LABELS)),
    })
    
    random.shuffle(seqs)
    n = len(seqs)
    n_train = int(n*0.8)
    n_valid = int(n*0.1)
    train = seqs[:n_train]
    valid = seqs[n_train:n_train + n_valid]
    test = seqs[n_train + n_valid:]
    
    ds = DatasetDict({
        "train": Dataset.from_dict(to_dict_format(train), features=features),
        "validation": Dataset.from_dict(to_dict_format(valid), features=features),
        "test": Dataset.from_dict(to_dict_format(test), features=features),
    })

    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(HF_DATASET_DIR):
        os.makedirs(HF_DATASET_DIR)
        
    ds.save_to_disk(HF_DATASET_DIR)
    
    print("---")
    print("âœ… BIO+Dataset ìƒì„± ì™„ë£Œ!")
    print(f"train={len(train)}, valid={len(valid)}, test={len(test)}")
    print(f"HuggingFace Dataset ì €ì¥ ìœ„ì¹˜: {HF_DATASET_DIR}")
    print("---")
    
    # --------------------------
    # í›„ì²˜ë¦¬ ë¡œì§ ì˜ˆì‹œ
    # ì´ ë¶€ë¶„ì€ ì‹¤ì œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼(predicted_tags)ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    # --------------------------
    
    def predict_ner_tags(tokens: List[str]) -> List[int]:
        """
        ì£¼ì–´ì§„ í† í°ì— ëŒ€í•´ NER íƒœê·¸ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜.
        ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë¡œì§ìœ¼ë¡œ êµì²´ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        # ì˜ˆì¸¡ ë¡œì§ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        # 'B-ORG'ì— í•´ë‹¹í•˜ëŠ” 'ì‚¼ì„±'ì€ 1, 'O'ì— í•´ë‹¹í•˜ëŠ” 'ì‚¬ì´ë²„'ëŠ” 0 ë“±
        predictions_map = {
            "ì‚¼ì„±": 1,         # B-ORG
            "ì‚¬ì´ë²„": 0,       # O
            "ê³µê²©": 5,         # B-ATTACK
            "ì œë¡œë°ì´": 3,     # B-VULN
            "ì·¨ì•½ì ": 4,       # I-VULN
            "ì•ˆë©": 1,         # B-ORG
            "AhnLab": 1,       # B-ORG
        }
        
        # ì˜ˆì¸¡ íƒœê·¸ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™” (ëª¨ë‘ 'O' íƒœê·¸)
        predicted_tags = [0] * len(tokens)
        
        # ì˜ˆì¸¡ ë§µì„ ê¸°ë°˜ìœ¼ë¡œ íƒœê·¸ í• ë‹¹
        for i, token in enumerate(tokens):
            if token in predictions_map:
                predicted_tags[i] = predictions_map[token]
            
        return predicted_tags

    print("\n[í›„ì²˜ë¦¬ ë¡œì§ ì˜ˆì‹œ - ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬]")
    # ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œëœ í† í°ì´ë¼ê³  ê°€ì •
    example_tokens = ["ì‚¼ì„±", "ì´", "ì‚¬ì´ë²„", "ê³µê²©", "ìœ¼ë¡œ", "ì œë¡œë°ì´", "ì·¨ì•½ì ", "ì„", "ë°œê²¬í–ˆë‹¤", "."]
    
    # 'predict_ner_tags' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    # ì‹¤ì œë¡œëŠ” ì´ ë¶€ë¶„ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì˜ˆì¸¡ì„ ì‹¤í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.
    predicted_tags = predict_ner_tags(example_tokens)

    print(f"ì˜ˆì¸¡ëœ íƒœê·¸: {predicted_tags}")
    
    # í›„ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
    processed_keywords = postprocess_ner_results(example_tokens, predicted_tags)
    
    print("ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬ ì™„ë£Œ:")
    for tag_type, keywords in processed_keywords.items():
        if keywords:
            print(f"- {tag_type}: {keywords}")